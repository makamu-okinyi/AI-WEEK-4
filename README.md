#  Week 4 Assignment(PART 2 PRACTICAL IMPLEMENTATION): Building Intelligent Software Solutions

This repository contains the practical implementation (Part 2) for the Week 4 AI in Software Engineering assignment. The project demonstrates how Artificial Intelligence and Machine Learning tools can automate tasks, enhance decision-making, and improve code quality in the software development lifecycle.

---

##  Task 1: AI-Powered Code Completion Analysis

This task compares a manually written function against code generated by an AI completion tool (e.g., GitHub Copilot) to sort a list of dictionaries.

###  Objective

To assess the efficiency, correctness, and development speed gains provided by AI code generation.

### Implementation (Python)

| Implementation Type | Code Snippet | Key Feature |
| :--- | :--- | :--- |
| **Manual** | `sorted(list_of_dicts, key=lambda d: d[key_to_sort_by])` | Explicit developer writing, including error handling. |
| **AI-Suggested** | `return sorted(data, key=lambda x: x[key])` | Rapid generation, often including robust features like `try...except` blocks for `KeyError` and `TypeError`. |

###  Analysis and Efficiency (200-Word Summary)

The AI-suggested code demonstrated a **significant reduction in development time** (near-instantaneous generation) compared to manual implementation. Both versions used the highly efficient C-implemented `sorted()` function, making their runtime performance (O($N \log N$)) functionally identical.

However, the AI solution was often superior in **code quality and robustness** because it automatically included best-practice elements like `try...except` blocks for handling missing keys and incomparable data types. While the manual version requires a developer to remember to add these checks, the AI provided a safer, production-ready function immediately, resulting in greater overall development efficiency.

---

##  Task 2: Automated Testing with AI

This task involved automating a standard test case for a login page to assess the role of automation and AI in improving test coverage and maintainability.

###  Objective

Automate valid and invalid login test cases and analyze how AI-driven tools mitigate test maintenance costs.

###  Implementation (Python/Selenium)

The tests were executed using Python with the **Selenium WebDriver** and `webdriver-manager` libraries (run in a headless configuration on Google Colab). The script tested a sample login page using predefined valid and invalid credentials and verified the resulting success or error messages.

**Deliverable Screenshot:** The results of the test script, showing the execution summary and final rates. 
![screenshot1](https://imgur.com/3b7dJmk.png) |  ![screenshot2](https://imgur.com/9fFfQt8.png)

 |  ![screenshot3](https://imgur.com/NuoPkzL.png)

| Test Case | Result | Success Rate |
| :--- | :--- | :--- |
| Valid Credentials | Passed (Redirected to secure area) | 100% |
| Invalid Credentials | Passed (Displayed error message) | 100% |
| **Overall Rate** | N/A | **100.00%** |

###  AI Impact on Test Coverage (150-Word Summary)

AI improves test coverage primarily by addressing test **brittleness** and **maintenance**. Traditional automated tests (like the pure Selenium script) rely on hardcoded locators (e.g., `By.ID`), which break easily when the UI changes, resulting in zero coverage for that feature until manually fixed.

AI-powered frameworks (like Testim.io) use **self-healing locators**. These ML models learn multiple element attributes (ID, text, position, relationship to surrounding elements). If one attribute changes, the AI automatically updates the locator to maintain the test's functionality. This ensures tests **continually run** and provide valid coverage, leading to a more reliable and expansive test suite than is possible with time-consuming manual maintenance.

---

##  Task 3: Predictive Analytics for Resource Allocation

This task demonstrates how a Machine Learning model can be deployed to make resource allocation decisions by predicting the priority of a hypothetical "GitHub Issue."

###  Objective

Preprocess a dataset, train a **Random Forest Classifier**, and evaluate its performance in a multi-class classification scenario.

###  Implementation (Jupyter Notebook)

The `load_breast_cancer` dataset was used as a proxy for complex issue metrics. A custom multi-class target variable ('Low,' 'Medium,' 'High' Priority) was engineered to simulate a real-world resource allocation problem.

![SCREENSHOT](https://imgur.com/4896Rnr.png)
 A screenshot of the Jupyter Notebook cell showing the final output with the calculated performance metrics. 

| Step | Technique Used | Purpose |
| :--- | :--- | :--- |
| **Preprocessing** | Feature Engineering, `train_test_split`, `stratify` | Created realistic multi-class labels and ensured balanced training/testing data splits. |
| **Modeling** | **Random Forest Classifier** (`n_estimators=100`) | A robust ensemble learning method chosen for high performance and resistance to overfitting in classification tasks. |
| **Evaluation** | `accuracy_score`, `f1_score`, `classification_report` | Assessed the model's predictive reliability across all three priority classes. |

###  Performance Metrics

The model demonstrated strong predictive capabilities for issue priority:

| Metric | Result | Interpretation |
| :--- | :--- | :--- |
| **Accuracy Score** | **0.9649** | The percentage of issues correctly assigned the right priority label. |
| **F1-Score (Weighted)** | **0.9632** | The harmonic mean of precision and recall, providing a balanced measure of performance across all priority classes. |

This model can be deployed to automatically triage incoming issues, reducing manual overhead and intelligently allocating engineering resources.






